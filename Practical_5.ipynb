{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Practical 5: CBOW implementation (Colab-ready)\n",
        "# Code extracted from Prac_5.pdf (formatting fixed for Colab)\n",
        "# =========================\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Lambda, Dense, Input\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "corpus = \"\"\"Natural language processing enables computers to understand human language.\n",
        "Deep learning models like CBOW help us learn word representations.\n",
        "Word embeddings capture semantic meaning in vector space.\n",
        "This makes NLP applications like translation and sentiment analysis possible.\n",
        "The CBOW model predicts a word based on its context words.\"\"\"\n",
        "\n",
        "\n",
        "sentences = [s.strip().lower() for s in corpus.split('.') if s.strip()]\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "word_index = tokenizer.word_index\n",
        "index_word = {v: k for k, v in word_index.items()}\n",
        "index_word[0] = \"<PAD>\"\n",
        "\n",
        "print(\"Vocabulary:\", word_index)\n",
        "print(\"Sequences:\", sequences)\n",
        "print(\"Vocab Size:\", vocab_size)\n",
        "\n",
        "\n",
        "def generate_cbow_pairs(sequences, window_size=2):\n",
        "    contexts, targets = [], []\n",
        "    context_len = window_size * 2\n",
        "\n",
        "    for seq in sequences:\n",
        "        for i, target in enumerate(seq):\n",
        "            context = []\n",
        "            for j in range(i - window_size, i + window_size + 1):\n",
        "                if j == i:\n",
        "                    continue\n",
        "                if 0 <= j < len(seq):\n",
        "                    context.append(seq[j])\n",
        "                else:\n",
        "                    context.append(0)\n",
        "            contexts.append(context)\n",
        "            targets.append(target)\n",
        "\n",
        "    return np.array(contexts), np.array(targets)\n",
        "\n",
        "\n",
        "X, y = generate_cbow_pairs(sequences, window_size=2)\n",
        "\n",
        "print(\"\\nContext shape:\", X.shape)\n",
        "print(\"Target shape:\", y.shape)\n",
        "print(\"Example contexts -> targets:\")\n",
        "for i in range(min(5, len(X))):\n",
        "    print([index_word[idx] for idx in X[i]], \"->\", index_word[y[i]])\n",
        "\n",
        "\n",
        "embedding_dim = 50\n",
        "context_len = X.shape[1]\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(context_len,)))\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=context_len, name=\"embedding\"))\n",
        "model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "history = model.fit(X, y, epochs=80, batch_size=16, verbose=2)\n",
        "\n",
        "\n",
        "embeddings = model.get_layer(\"embedding\").get_weights()[0]\n",
        "print(\"\\nEmbedding matrix shape:\", embeddings.shape)\n",
        "\n",
        "\n",
        "def predict_word(context_words):\n",
        "\n",
        "    context_indices = [word_index.get(w, 0) for w in context_words]\n",
        "    context_indices = np.array(context_indices).reshape(1, -1)\n",
        "    probs = model.predict(context_indices, verbose=0)[0]\n",
        "    pred_idx = np.argmax(probs)\n",
        "    return index_word[pred_idx], float(probs[pred_idx])\n",
        "\n",
        "\n",
        "context = [\"deep\", \"models\", \"cbow\", \"help\"]\n",
        "pred_word, prob = predict_word(context)\n",
        "print(\"\\nContext:\", context)\n",
        "print(\"Predicted Word:\", pred_word, \"with probability:\", prob)\n",
        "\n",
        "\n",
        "def nearest_words(word, top_k=5):\n",
        "    if word not in word_index:\n",
        "        return []\n",
        "    w_idx = word_index[word]\n",
        "    vec = embeddings[w_idx]\n",
        "    norms = np.linalg.norm(embeddings, axis=1)\n",
        "    sims = embeddings.dot(vec) / (norms * np.linalg.norm(vec) + 1e-9)\n",
        "    top = np.argsort(-sims)[1: top_k+1]  # skip the word itself\n",
        "    return [(index_word[i], float(sims[i])) for i in top]\n",
        "\n",
        "print(\"\\nNearest words to 'learning':\", nearest_words(\"learning\"))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rFbBWIWHpP4c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}